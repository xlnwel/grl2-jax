---
algorithm: &algo ppo
name: ppo
info: ppo

precision: 32

# model path: root_dir/model_name/name
# tensorboard path: root_dir/model_name/logs
# the following names are just examples; they will be re-specified in the entry point
root_dir: *algo
model_name: *algo

routine:
  algorithm: *algo

  MAX_STEPS: 2e7
  n_steps: &nsteps 200
  LOG_PERIOD: 5e4
  EVAL_PERIOD: null

  n_eval_envs: 100
  RECORD_VIDEO: False
  N_EVAL_EPISODES: 1
  size: [256, 256]

  compute_return_at_once: False
  perm: null

env:
  env_name: &env_name grf-academy_3_vs_1_with_keeper
  representation: simple115v2
  render: False
  write_full_episode_dumps: False
  write_video: False
  dump_frequency: 1000
  logdir: results/grf
  extra_players: null
  control_left: True
  control_right: False
  shared_policy: True
  score_reward_scale: 1
  reward_type: shared

  max_episode_steps: 200
  use_action_mask: False
  uid2aid: null

  n_runners: &nrunners 10
  n_envs: &nenvs 5

  use_idx: False
  timeout_done: &td True

  record_prev_action: True

agent: {}

monitor:
  use_tensorboard: True

strategy:
  algorithm: *algo
  train_loop: {}

model:
  aid: 0
  gamma: &gamma .99

  policy:
    nn_id: policy
    units_list: [64, 64]
    w_init: orthogonal
    activation: relu
    norm: layer
    norm_after_activation: True
    out_scale: .01
    rnn_type: &prnn lstm
    rnn_units: 64
    rnn_init: null
    rnn_norm: layer
    use_feature_norm: False
  value:
    nn_id: value
    units_list: [64, 64]
    w_init: orthogonal
    activation: relu
    norm: layer
    norm_after_activation: True
    rnn_type: &vrnn lstm
    rnn_units: 64
    rnn_init: null
    rnn_norm: layer
    use_feature_norm: False

loss:
  # hyperparams for value target and advantage
  target_type: gae
  c_clip: 1
  rho_clip: 1
  adv_type: gae
  norm_adv: True

  prnn_bptt: 10
  vrnn_bptt: 10

  # hyperparams for policy optimization
  pg_type: ppo
  opt_pg: False
  ppo_clip_range: .2
  teammate_ratio_clip: .2
  policy_sample_mask: True

  # hyperparams for value learning
  value_loss: clip_huber
  value_clip_range: .2
  huber_threshold: 10
  value_sample_mask: False
  popart: &popart True

  stats:
    gamma: *gamma
    lam: &lam .95
    pg_coef: 1
    entropy_coef: 1e-2
    value_coef: 1

trainer:
  algorithm: *algo
  aid: 0
  n_runners: *nrunners
  n_envs: *nenvs
  n_epochs: &nepochs 15
  n_mbs: &nmbs 1
  n_steps: *nsteps
  update_scheme: whole
  popart: *popart
  debug: False

  policy_opt:
    opt_name: adam
    lr: 5e-4
    clip_norm: 10
    eps: 1e-5
  value_opt:
    opt_name: adam
    lr: 5e-4
    clip_norm: 10
    eps: 1e-5

actor:
  update_obs_rms_at_execution: False
  update_obs_at_execution: False
  rms:
    obs:
      obs_names: [obs, global_state]
      obs_normalized_axis: [0, 1]  # obs is normalized at once with the sequential dimension
      obs_normalized_ndim: 2
      normalize_obs: True
      use_feature_mask: True
      obs_clip: 10
    reward:
      reward_normalized_axis: [0, 1] # reward is normalized at once with the sequential dimension
      reward_normalized_ndim: 1
      normalize_reward: False
      update_reward_rms_in_time: False
      gamma: *gamma

buffer:
  type: ac

  n_runners: *nrunners
  n_envs: *nenvs
  n_steps: *nsteps
  queue_size: 2
  timeout_done: *td

  gamma: *gamma
  lam: *lam

  sample_keys: 
    - obs
    - global_state
    - action
    - value
    - reward
    - discount
    - reset
    - mu_logprob
    - mu_logits
    - state_reset
    - state
