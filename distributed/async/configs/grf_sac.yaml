---
algorithm: &algo async-sac
name: async-sac
info: async-sac

precision: 32

n_agents: 1
self_play: False
is_ma_algo: True

# model path: root_dir/model_name/name
# tensorboard path: root_dir/model_name/logs
# the following names are just examples; they will be re-specified in the entry point
root_dir: *algo
model_name: *algo

controller:
  store_period: 5e4
  restart_runners_period: null
  max_version_iterations: 1
  max_steps_per_iteration: &ms 5e6
  initialize_rms: &irms False

parameter_server:
  root_dir: *algo
  model_name: *algo

  train_from_scratch_frac: 1
  online_frac: .2

  payoff:
    step_size: 1e-2   # step size towards the most recent data, 0 or null average payoff over the entire history
    update_interval: 180
    sampling_strategy:
      type: pfsp
      p: 1
      threshold: .3

  # rule_strategies:
  #   # agent name / config
  #   random:
  #     aid: 1
  #     vid: 1
  #     path: rule/random   # path to the file that defines Strategy
  #     # other configs

ray_config:
  runner:
    num_cpus: 1
  agent:
    num_gpus: 0

monitor: {}

runner:
  n_runners: &nrunners 10
  n_steps: &nsteps 10
  push_every_episode: False
  algo_type: offline

env:
  env_name: &env_name grf-academy_pass_and_shoot_with_keeper
  representation: simple115v2
  render: False
  write_full_episode_dumps: False
  write_video: False
  dump_frequency: 1000
  logdir: results/grf
  extra_players: null
  control_left: True
  control_right: False
  shared_policy: True
  score_reward_scale: 1

  max_episode_steps: 200
  use_action_mask: False
  uid2aid: null

  n_envs: &nenvs 10

  use_idx: False
  timeout_done: &td True

agent: {}

strategy:
  algorithm: *algo
  train_loop:
    n_epochs: 100

model:
  aid: 0
  gamma: &gamma .99
  polyak: .995
  n_Qs: 2

  policy:
    nn_id: policy
    units_list: [512, 512]
    w_init: orthogonal
    activation: relu
    norm: null
    rnn_type: &prnn null
    rnn_units: 64
  Q:
    nn_id: Q
    units_list: [512, 512]
    w_init: orthogonal
    activation: relu
    norm: null
    rnn_type: &vrnn null
    rnn_units: 64
  temp:
    nn_id: temp
    type: constant
    value: .2

loss:
  prnn_bptt: 10
  vrnn_bptt: 10

  stats:
    gamma: *gamma
    policy_coef: 1
    q_coef: 1
    temp_coef: 1

trainer:
  algorithm: *algo
  aid: 0

  policy_opt:
    opt_name: adam
    lr: 5e-4
    clip_norm: 10
    eps: 1e-5
  Q_opt:
    opt_name: adam
    lr: 5e-4
    clip_norm: 10
    eps: 1e-5
  temp_opt:
    opt_name: adam
    lr: 5e-4
    clip_norm: 10
    eps: 1e-5

actor: {}

buffer:
  type: uniform
  data_type: step

  n_runners: *nrunners
  n_envs: *nenvs
  max_size: 1e6
  min_size: 1e3
  batch_size: 100
  sample_size: 1

  max_steps: 1
  gamma: *gamma

  sample_keys:
    - obs
    - global_state
    - action
    - reward
    - discount
    - steps
