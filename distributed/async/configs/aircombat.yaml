---
# NOTE: root_dir and model_name will be specified to all configs in run.train.py
algorithm: &algo async-ppo
name: async-ppo
info: async-ppo
version: 0

precision: 32

n_agents: 2
self_play: True

# model path: root_dir/model_name/name
# tensorboard path: root_dir/model_name/logs
# the following names are just examples; they will be re-specified in the entry point
root_dir: *algo
model_name: *algo

controller:
  store_period: 1e5
  reload_strategy_pool_period: 1e4
  restart_runners_period: null
  eval_period: null
  max_version_iterations: 100
  max_steps_per_iteration: 5e7
    # - [0, 1e6]
    # - [20, 3e6]
  score_threshold: .6
  initialize_rms: &irms True

parameter_server:
  root_dir: *algo
  model_name: *algo

  score_metric: score   # stats recorded in payoff tables
  train_from_scratch_frac: .1
  train_from_latest_frac: 0
  hist_train_scale: 10
  reset_policy_head_frac: .5
  online_frac:
    - [1, 1]
    - [5, .75]
    - [10, .5]
    - [20, .25]
  exploiter_online_frac: .2
  hard_frac: .2
  hard_threshold: .5
  n_hard_opponents: 5

  pool_pattern: .*
  pool_name: strategy_pool

  payoff:
    step_size: 1e-1
    sampling_strategy:
      type: pfsp
      p: 1
      threshold: .3

  # rule_strategies:
  #   # agent name / config
  #   random:
  #     aid: 0
  #     vid: 1
  #     path: rule/random   # path to the file that defines Strategy
  #     # other configs

ray_config:
  runner:
    num_cpus: 1
  agent:
    num_cpus: 2
    num_gpus: .5

monitor:
  print_terminal_info: True

runner:
  n_runners: 90
  n_steps: &nsteps 200
  push_every_episode: False
  algo_type: onpolicy

env:
  env_name: &env_name aircombat-1v1
  n_envs: &nenvs 1
  shared_policy: False
  multi_agent: True
  timeout_done: &td True

  plane_config:
    sim_step: 0.05
    auto_throttle: False
    n_missile: 4
    launch_interval: 25
    ctrl_mod: dFLon_dLat
    attack_reward_scale: 5
    missile_terminal_penalty_scale: 1
    escape_reward_scale: 5
    heading_reward_scale: .001
    dist_reward_scale: .0001
    control_fire: True
    escape_dis: False
  win_loss_reward: 20
  max_sim_time: 600
  frame_skip: 8
  n_bins: null
  born_point:
    pn: 
      - [-50000, 50000]
      - [-50000, 50000]
    pe: 
      - [15000, 50000]
      - [-50000, -15000]
    alt: 
      - [5000, 8000]
      - [5000, 8000]
    heading: 
      - [-180, 180]
      - [-180, 180]
    vt: 
      - [280, 340]
      - [280, 340]
  
agent: {}

strategy:
  algorithm: *algo
  train_loop: {}

model:
  aid: 0
  gamma: &gamma .995
  eval_act_temp: 1
  print_for_debug: False

  policy:
    nn_id: policy
    units_list: [256, 256]
    w_init: orthogonal
    activation: relu
    norm: layer
    norm_after_activation: True
    out_scale: .01
    rnn_type: &prnn lstm
    rnn_units: 256
    rnn_init: null
    rnn_norm: layer
    out_act: tanh
    init_std: .2
    sigmoid_scale: True
    std_x_coef: 1.
    std_y_coef: .5
    use_feature_norm: False
  value:
    nn_id: value
    units_list: [256, 256]
    w_init: orthogonal
    activation: relu
    norm: layer
    norm_after_activation: True
    rnn_type: &vrnn lstm
    rnn_units: 256
    rnn_init: null
    rnn_norm: layer
    use_feature_norm: False

loss:
  # hyperparams for value target and advantage
  target_type: gae
  c_clip: 1
  rho_clip: 1
  adv_type: gae
  norm_adv: True

  prnn_bptt: 10
  vrnn_bptt: 10

  # hyperparams for policy optimization
  pg_type: ppo
  opt_pg: False
  ppo_clip_range: .2
  policy_sample_mask: True

  # hyperparams for value learning
  value_loss: clip_huber
  value_clip_range: .2
  huber_threshold: 10
  value_sample_mask: False
  popart: &popart True

  # hyperparams for meta-learning
  stats:
    gamma: *gamma
    lam: &lam .95
    pg_coef: 1
    entropy_coef: 1e-4
    value_coef: 1

trainer:
  algorithm: *algo
  aid: 0
  n_runners: &nrunners 10
  n_envs: *nenvs
  n_epochs: &nepochs 1
  n_mbs: &nmbs 1
  n_steps: *nsteps     # BPTT length
  popart: *popart
  debug: True

  policy_opt:
    opt_name: adam
    lr: 5e-4
    clip_norm: 10
    eps: 1e-5
  value_opt:
    opt_name: adam
    lr: 5e-4
    clip_norm: 10
    eps: 1e-5

actor:
  update_obs_rms_at_execution: False
  update_obs_at_execution: False
  rms:
    obs:
      obs_names: [obs, global_state]
      obs_normalized_axis: [0, 1]  # obs is normalized at once with the sequential dimension
      obs_normalized_ndim: 2
      normalize_obs: *irms
      obs_clip: 10
    reward:
      reward_normalized_axis: [0, 1] # reward is normalized at once with the sequential dimension
      reward_normalized_ndim: 1
      normalize_reward: False
      update_reward_rms_in_time: False
      gamma: *gamma

buffer:
  type: ac

  n_runners: *nrunners
  n_envs: *nenvs
  n_steps: *nsteps
  queue_size: 2
  timeout_done: *td
  gamma: *gamma
  lam: *lam

  # mini-batch size = n_runners * n_envs * epslen / n_mbs
  sample_keys:
    - obs
    - global_state
    - action
    - value
    - reward
    - discount
    - reset
    - mu_logprob
    - mu_logits
    - state_reset
    - state
